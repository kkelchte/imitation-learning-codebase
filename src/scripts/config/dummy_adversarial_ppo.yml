architecture_config:
  architecture: adversarial_actor_critic
  device: cpu
  log_std: -0.5
  random_seed: 5100
  initialisation_type: orthogonal
data_saver_config:
  clear_buffer_before_episode: true
  store_on_ram_only: true
environment_config:
  normalize_observations: false
  normalize_rewards: false
  invert_reward: false
  factory_key: GYM
  gym_config:
    random_seed: 5346
    render: false
    world_name: tracking-v0
    args: 'random_location=True'
  max_number_of_steps: 300  # let number of steps depend on environment
episode_runner_config:
  train_every_n_steps: 1000
number_of_epochs: 5000
run_test_episodes: true
output_path: test_adversarial
save_checkpoint_every_n: 100
tb_render_every_n_epochs: 10
tensorboard: true
trainer_config:
  criterion: MSELoss
  critic_learning_rate: 0.0001
  actor_learning_rate: 0.00001
  gradient_clip_norm: 3
  optimizer: RMSprop
  data_loader_config:
    batch_size: 1000
    random_seed: 5346
  device: cpu
  discount: 0.99
  factory_key: APPO
  gae_lambda: 0.95
  phi_key: gae
  entropy_coefficient: 0.01
  max_actor_training_iterations: 50
  max_critic_training_iterations: 10
  epsilon: 0.1
  kl_target: 0.01
  use_kl_stop: false