architecture_config:
  architecture: pendulum_3_1c_stochastic
  device: cuda
  random_seed: 123
  initialisation_type: orthogonal
data_saver_config:
  clear_buffer_before_episode: true
  store_on_ram_only: true
environment_config:
  normalize_observations: true
  normalize_rewards: true
  observation_clipping: 5
  reward_clipping: 5
  factory_key: GYM
  gym_config:
    random_seed: 123
    render: false
    world_name: Pendulum-v0
  max_number_of_steps: -1  # let number of steps depend on environment
episode_runner_config:
  number_of_episodes: -1  # stop when there are enough samples in buffer to fill batch
number_of_epochs: 1000
output_path: pendulum/normalized_reward_observations_entropy
tensorboard: true
save_checkpoint_every_n: 100
trainer_config:
  criterion: MSELoss
  learning_rate: 0.0001
  gradient_clip_norm: 1
  optimizer: Adam
  data_loader_config:
    batch_size: 2048
    random_seed: 123
  device: cuda
  discount: 0.99
  factory_key: PPO
  gae_lambda: 0.95
  phi_key: gae
  entropy_coefficient: 0.
  max_actor_training_iterations: 10
  max_critic_training_iterations: 10
  epsilon: 0.2
  kl_target: 0.01